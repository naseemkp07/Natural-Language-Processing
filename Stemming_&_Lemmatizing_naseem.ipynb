{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn2wnt4tSvgf8fw4Vp+hHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naseemkp07/Natural-Language-Processing/blob/main/Stemming_%26_Lemmatizing_naseem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "3Xj_zaszM503"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea of stemming is a sort of normalizing method.\n",
        "\n",
        " Many variations of words carry the same meaning, other than when tense is involved.\n",
        "\n",
        " The reason why we stem is to shorten the lookup, and normalize sentences.\n",
        "\n",
        "# Consider:\n",
        " I was taking a ride in the car.\n",
        "\n",
        " I was riding in the car.\n",
        "\n",
        "This sentence means the same thing. in the car is the same.\n",
        "\n",
        " I was is the same. the ing denotes a clear past-tense in both cases\n",
        "\n",
        " so is it truly necessary to differentiate between ride and riding,\n",
        " in the case of just trying to figure out the meaning of what this past-tense activity was"
      ],
      "metadata": {
        "id": "5C6ASmVINHkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WordNet is a lexical database for the English language, often used in natural language processing and computational linguistics.\n",
        "# It's a large, machine-readable database of words and their semantic relationships.\n",
        "# you can use the WordNetLemmatizer to find the base or dictionary form (lemma) of words based on their part of speech.\n",
        "\n",
        "\n",
        "# The punkt module in NLTK refers to a tokenizer for splitting text into sentences.\n",
        "# Tokenization is the process of breaking down a text into smaller units, which can be words or sentences, depending on the level of tokenization."
      ],
      "metadata": {
        "id": "bskBg8dmLPNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0_Q5CD9LPKq",
        "outputId": "3a6650c1-d720-4dc5-b40d-7e55d8a14bb9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fzzPueXLPDa",
        "outputId": "b3a033d8-93aa-4f7a-d3cb-54cb68c4ca6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "ps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH7UY4JlLO9H",
        "outputId": "1f94774d-e063-4eac-adfd-fa71fb5b4b76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PorterStemmer>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets choose some similar stem words\n",
        "example_words = [\"address\",\"administration\",\"apartment\",\"sleeping\",\"analysis\",\"Stemming\"]\n",
        "print('Stemming word Example')\n",
        "print(example_words)\n",
        "print('\\n')\n",
        "print('After Stemming, Words are')\n",
        "# Next, we can easily stem by doing something like:\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnjz2IEQLO7S",
        "outputId": "ed7a19ae-18e9-47bb-bc60-155fd9ca0281"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming word Example\n",
            "['address', 'administration', 'apartment', 'sleeping', 'analysis', 'Stemming']\n",
            "\n",
            "\n",
            "After Stemming, Words are\n",
            "address\n",
            "administr\n",
            "apart\n",
            "sleep\n",
            "analysi\n",
            "stem\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's try stemming a typical sentence, rather than some words:\n",
        "\n",
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "print('Another Example')\n",
        "print(new_text)\n",
        "print('\\n')\n",
        "\n",
        "print('For each word, stemming done as follows')\n",
        "# Word Tokenizer\n",
        "words = word_tokenize(new_text)\n",
        "# For each word, stemming done\n",
        "for w in words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP_tzh3CLOxv",
        "outputId": "dbcca816-3f28-4891-90d4-71ab94d6ca96"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another Example\n",
            "It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\n",
            "\n",
            "\n",
            "For each word, stemming done as follows\n",
            "it\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing\n",
        "\n",
        "# A very similar operation to stemming is called lemmatizing.\n",
        "# The major difference between these is, as you saw earlier,\n",
        "# stemming can often create non-existent words, whereas lemmas are actual words.\n",
        "# So, your root stem, meaning the word you end up with,\n",
        "# is not something you can just look up in a dictionary, but you can look up a lemma.\n",
        "# Some times you will wind up with a very similar word, but sometimes,\n",
        "# you will wind up with a completely different word"
      ],
      "metadata": {
        "id": "Z5Tt8YwJLOvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"cats\"))\n",
        "print(lemmatizer.lemmatize(\"careful\"))\n",
        "print(lemmatizer.lemmatize(\"geese\"))\n",
        "print(lemmatizer.lemmatize(\"battery\"))\n",
        "print(lemmatizer.lemmatize(\"challenges\"))\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"chicken\"))\n",
        "print(lemmatizer.lemmatize(\"ran\",'v'))\n",
        "\n",
        "\n",
        "\n",
        "# Inference :\n",
        "# But lemmatization has limits.\n",
        "# For example, Porter stems both happiness and happy to happi,\n",
        "# while WordNet lemmatizes the two words to themselves.\n",
        "# The WordNet lemmatizer also requires specifying the word’s part of speech\n",
        "# otherwise, it assumes the word is a noun.\n",
        "# Finally, lemmatization cannot handle unknown words: for example,\n",
        "# Porter stems both iphone and iphones to iphon, while WordNet lemmatizes both words to themselves.\n",
        "# In the example of shoe and shoes, we probably want to treat the two forms identically.\n",
        "# But we wouldn’t want to do the same for the words logistic(Mathematical term) and logistics(Mechanical term),\n",
        "# which mean very different things despite their apparent similarity.\n",
        "# Nor would we want to equate the words universe and university,\n",
        "# even though both words derive from the same Latin root."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H25tfvpXLOlg",
        "outputId": "f9a14b34-fea4-4834-d24c-3cfe5b178b10"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "careful\n",
            "goose\n",
            "battery\n",
            "challenge\n",
            "good\n",
            "best\n",
            "chicken\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"happier\", pos=\"a\"))  # Output: happy ,\n"
      ],
      "metadata": {
        "id": "UN2VUp-kMD8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"children\", pos=\"n\"))  # Output: child\n"
      ],
      "metadata": {
        "id": "AtAlmgcLMFZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos=part of speech\n",
        "# a=adverb\n",
        "# n=noun\n",
        "# v=verb"
      ],
      "metadata": {
        "id": "EypdvEVGMFWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZwug_E5MFMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePIcvoQuMEgI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}